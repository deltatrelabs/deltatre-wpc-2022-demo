{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements\n",
    "\n",
    "Use **Python v3.8.x**, in a dedicated Virtual Environment and clone Yolov5 from official repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../src/yolov5'):\n",
    "    !git clone https://github.com/ultralytics/yolov5.git ../src/yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the following commands from the notebook. In case of errors, try to run them from the Terminal and restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -U pip\n",
    "%pip install wheel\n",
    "%pip install azureml-sdk[notebooks]\n",
    "%pip install click matplotlib numpy opencv-python\n",
    "%pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "%pip install -r ../src/yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import cv2\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core import Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.core.environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to AzureML workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only the first time, to login to your Azure Subscription, and then save in the config file, for future uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "\n",
    "ws = Workspace(subscription_id=\"YOUR-SUBSCRIPTION-ID\",\n",
    "               resource_group=\"YOUR-RESOURCE_GROUP\",\n",
    "               workspace_name=\"YOUR-AZUREML-WORKSPACE\",\n",
    "               auth=interactive_auth)\n",
    "\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For future uses, load auth settings from config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'yolov5_custom_training'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure compute resources (CPU/GPU cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpu-compute-k80\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 1)\n",
    "\n",
    "# This example uses GPU VM.\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"Standard_NC6\")\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes,\n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # Create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"PATH-TO-YOLOv5-Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not already done, upload the local dataset to the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.File.upload_directory(src_dir=data_folder, target=DataPath(datastore, \"datasets/yolov5_soccer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom classes\n",
    "class_names = [\n",
    "    \"unknown\", \"ball\", \"person\"\n",
    "]\n",
    "\n",
    "# Function to convert bounding boxes in YOLO format to xmin, ymin, xmax, ymax.\n",
    "def yolo2bbox(bboxes):\n",
    "    xmin, ymin = bboxes[0]-bboxes[2]/2, bboxes[1]-bboxes[3]/2\n",
    "    xmax, ymax = bboxes[0]+bboxes[2]/2, bboxes[1]+bboxes[3]/2\n",
    "    return xmin, ymin, xmax, ymax\n",
    "\n",
    "def plot_box(image, bboxes, labels):\n",
    "    # Need the image height and width to denormalize the bounding box coordinates\n",
    "    h, w, _ = image.shape\n",
    "    for box_num, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = yolo2bbox(box)\n",
    "        # denormalize the coordinates\n",
    "        xmin = int(x1*w)\n",
    "        ymin = int(y1*h)\n",
    "        xmax = int(x2*w)\n",
    "        ymax = int(y2*h)\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (xmin, ymin), (xmax, ymax),\n",
    "            color=(0, 0, 255),\n",
    "            thickness=6\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            class_names[int(labels[box_num])],\n",
    "            (xmin+1, ymin-10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            3,\n",
    "            (0, 255, 0),\n",
    "            10\n",
    "        )\n",
    "    return image\n",
    "\n",
    "\n",
    "# Function to plot images with bounding boxes\n",
    "def plot(image_paths, label_paths, num_samples):\n",
    "    all_training_images = glob.glob(image_paths)\n",
    "    all_training_labels = glob.glob(label_paths)\n",
    "    all_training_images.sort()\n",
    "    all_training_labels.sort()\n",
    "\n",
    "    plt.figure(figsize=(21, 12))\n",
    "    for i in range(num_samples):\n",
    "        image = cv2.imread(all_training_images[i])\n",
    "        with open(all_training_labels[i], 'r') as f:\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            label_lines = f.readlines()\n",
    "            for label_line in label_lines:\n",
    "                label = label_line[0]\n",
    "                bbox_string = label_line[2:]\n",
    "                x_c, y_c, w, h = bbox_string.split(' ')\n",
    "                x_c = float(x_c)\n",
    "                y_c = float(y_c)\n",
    "                w = float(w)\n",
    "                h = float(h)\n",
    "                bboxes.append([x_c, y_c, w, h])\n",
    "                labels.append(label)\n",
    "        result_image = plot_box(image, bboxes, labels)\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(result_image[:, :, ::-1])\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(\n",
    "    image_paths=os.path.join(data_folder,'images/train/*.jpg'),\n",
    "    label_paths=os.path.join(data_folder,'labels/train/*'),\n",
    "    num_samples=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a folder to be used as \"context\" for the training task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should point to Yolov5 cloned repository (https://github.com/ultralytics/yolov5)\n",
    "script_folder = os.path.join(os.getcwd(), \"..\\\\src\\\\yolov5\")\n",
    "script_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud environment\n",
    "\n",
    "Run the following cell to define the execution environment in the cloud compute instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Docker steps as a string.\n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.3-cudnn8-ubuntu20.04\n",
    "\n",
    "ENV PYTHON_VERSION 3.8\n",
    "\n",
    "RUN wget https://repo.anaconda.com/miniconda/Miniconda3-py38_4.11.0-Linux-x86_64.sh -O /tmp/miniconda.sh \\\n",
    "    && chmod +x /tmp/miniconda.sh \\\n",
    "    && /tmp/miniconda.sh -b -p /opt/miniconda3.8\n",
    "\n",
    "RUN export PATH=/opt/miniconda3.8/bin:$PATH \\\n",
    "    && python -m pip install --upgrade pip \\\n",
    "    && pip install wheel\n",
    "\n",
    "RUN export PATH=/opt/miniconda3.8/bin:$PATH \\\n",
    "    && apt-get update -qq  && apt-get install -y --no-install-recommends python3-opencv 2>&1 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN export PATH=/opt/miniconda3.8/bin:$PATH \\\n",
    "    && pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "RUN export PATH=/opt/miniconda3.8/bin:$PATH \\\n",
    "    && pip install \\\n",
    "    matplotlib \\\n",
    "    numpy \\\n",
    "    Pillow \\\n",
    "    PyYAML \\\n",
    "    requests \\\n",
    "    scipy \\\n",
    "    tqdm \\\n",
    "    tensorboard \\\n",
    "    pandas \\\n",
    "    seaborn \\\n",
    "    onnx \\\n",
    "    onnx-simplifier \\\n",
    "    thop \\\n",
    "    click \\\n",
    "    opencv-python \\\n",
    "    azureml-defaults\n",
    "\n",
    "RUN mv /opt/miniconda /opt/miniconda3.7 \\\n",
    "    && ln -s /opt/miniconda3.8 /opt/miniconda\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(\"cloud-env\")\n",
    "\n",
    "# Set the base image to None, because the image is defined by Dockerfile.\n",
    "env.docker.base_image = None\n",
    "env.docker.base_dockerfile = dockerfile\n",
    "\n",
    "env.python.user_managed_dependencies = True\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local compute target\n",
    "\n",
    "Run the following cell to execute the training script locally. The script will run in the local Python environment, so please configure all the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(\"local-env\")\n",
    "env.python.user_managed_dependencies = True\n",
    "env.python.interpreter_path = os.path.join(os.getcwd(), \"..\\\\.venv38\\\\Scripts\\python.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the AzureML training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following settings if training locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target = \"local\"\n",
    "dataset_path_val = data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following settings if training in the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.File.from_files(path=(datastore, 'datasets/yolov5_soccer'))\n",
    "dataset_path_val = dataset.as_named_input('input').as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = './soccer.yaml'\n",
    "weights_val = 'yolov5m.pt'\n",
    "img_val = 640\n",
    "epochs_val = 2\n",
    "batch_size_val = 16\n",
    "name_val = 'exp'\n",
    "project_val = './outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py',\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env,\n",
    "                      arguments=['--data', data_val, '--dataset_mount_path', dataset_path_val, '--weights', weights_val, '--img', img_val, '--epochs', epochs_val, '--batch-size', batch_size_val, '--project', project_val, '--name', name_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the compute target to the pipeline (if running in the cloud):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.run_config.target = compute_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=src)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training is completed, we can download the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_file(\"outputs/exp/weights/best.pt\", \"../src/yolov5/yolov5m_custom.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be used in the inference demo app, the model can be converted to ONNX format.  \n",
    "From within the virtual environment, from `src/yolov5` folder, launch:\n",
    "\n",
    "`python export.py --weights yolov5m_custom.pt --imgsz 640 640 --include onnx`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the ONNX model will be saved in the same folder, and can then be used in the demo .NET application, just define a new MLModel with the proper parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4cee0a834a28488fee97fc001aebafc1fb3e2fdfa1bf6d59bbbff1176bb2e92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
